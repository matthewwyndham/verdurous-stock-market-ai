{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Run me first to import and setup everything\n",
    "import time\n",
    "import csv  \n",
    "from myapikey import APIkey\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def add_classification(stocks):\n",
    "    new_stocks = []\n",
    "    last_price = stocks[len(stocks) - 1][4]\n",
    "    new_stocks.append(np.insert(stocks[len(stocks) - 1], 5, 1)) # up\n",
    "    for s in reversed(list(range(len(stocks) - 1))):\n",
    "        if (stocks[s][4] - last_price) > 0:\n",
    "            new_stocks.append(np.insert(stocks[s], 5, 1)) # up\n",
    "        else:\n",
    "            new_stocks.append(np.insert(stocks[s], 5, 0)) # down\n",
    "        last_price = stocks[s][4]\n",
    "    return new_stocks\n",
    "\n",
    "def split_sample(data):\n",
    "    sample = []\n",
    "    for start in range(0, len(data) - 50, 50):\n",
    "        sample.append(data[start:start+50])\n",
    "    return sample\n",
    "\n",
    "def prepare(group_of_stocks):\n",
    "    prepared_data_set = []\n",
    "    for stock in group_of_stocks:\n",
    "        prepared_data_set.append(split_sample(add_classification(stock[0].to_numpy())))\n",
    "    return prepared_data_set\n",
    "\n",
    "def prep_part_two(group_of_stocks):\n",
    "    new_set = []\n",
    "    for stock in group_of_stocks:\n",
    "        for collection in stock:\n",
    "            classification = collection[0][5]\n",
    "            training_data = collection[1:]\n",
    "            new_set.append([training_data, classification])\n",
    "    return new_set\n",
    "\n",
    "# def get_intraday(stock):\n",
    "#     ts = TimeSeries(key=APIkey, output_format='pandas', indexing_type='integer')\n",
    "#     return ts.get_intraday(symbol=stock, interval='1min', outputsize='full')\n",
    "\n",
    "def get_daily(stock):\n",
    "    ts = TimeSeries(key=APIkey, output_format='pandas')\n",
    "    data, meta_data = ts.get_daily(symbol=stock, outputsize='full')\n",
    "    return data, meta_data\n",
    "\n",
    "def open_csv(path_to_csv):\n",
    "    data = []\n",
    "    with open(path_to_csv) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            if row[0] != '':\n",
    "                data.append(row)\n",
    "    return data\n",
    "\n",
    "def batch_get_daily(stock_list):\n",
    "    print(\"total (\", len(stock_list), \")\")\n",
    "    big_counter = 0\n",
    "    count = 0\n",
    "    data = []\n",
    "    for stock in stock_list:\n",
    "        if count == 5:\n",
    "            time.sleep(65) # 65 seconds just in case the timing on the server or here isn't perfect\n",
    "            count = 0\n",
    "        data.append(get_daily(stock))\n",
    "        big_counter += 1\n",
    "        print(stock, \":\", big_counter, end=\" \")\n",
    "        count += 1\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "#######                           ########\n",
    "#######    WOULD LIKE TO HAVES    ########\n",
    "#######                           ########\n",
    "##########################################\n",
    "\n",
    "print(\"would like to haves\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save data as CSV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Wait for API (free) access limit\n",
    "import time\n",
    "\n",
    "def batch_get_daily(stock_list):\n",
    "    print(\"total (\", len(stock_list), \")\")\n",
    "    big_counter = 0\n",
    "    count = 0\n",
    "    data = []\n",
    "    for stock in stock_list:\n",
    "        if count == 5:\n",
    "            time.sleep(65) # 65 seconds just in case the timing on the server or here isn't perfect\n",
    "            count = 0\n",
    "        data.append(get_daily(stock))\n",
    "        big_counter += 1\n",
    "        print(stock, \":\", big_counter, end=\" \")\n",
    "        count += 1\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save results of Training\n",
    "# https://www.tensorflow.org/guide/keras#entire_model\n",
    "model.save('verdurouMKI.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load Results from Previous Training\n",
    "loaded_model = tf.keras.models.load_model('verdurouMKI.h5')\n",
    "print(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Scrape HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "############                  ############\n",
    "############    MUST HAVES    ############\n",
    "############                  ############\n",
    "##########################################\n",
    "\n",
    "print(\"Must Haves\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Read CSV\n",
    "# This will read in the CSVs that come from the alpha vantage website, where each CSV\n",
    "# is from a separate stock symbol\n",
    "import csv  \n",
    "\n",
    "def open_csv(path_to_csv):\n",
    "    data = []\n",
    "    with open(path_to_csv) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            if row[0] != '':\n",
    "                data.append(row)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Access API\n",
    "from myapikey import APIkey\n",
    "from alpha_vantage.timeseries import TimeSeries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# must provide an api key (they are free from the alpha vantage website\n",
    "# this determines how much data you can get in how much time.\n",
    "def get_intraday(stock):\n",
    "    ts = TimeSeries(key=APIkey, output_format='pandas', indexing_type='integer')\n",
    "    return ts.get_intraday(symbol=stock, interval='1min', outputsize='full')\n",
    "\n",
    "# provide an api key to access the data\n",
    "def get_daily(stock):\n",
    "    ts = TimeSeries(key=APIkey, output_format='pandas')\n",
    "    data, meta_data = ts.get_daily(symbol=stock, outputsize='full')\n",
    "    return data, meta_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499\n"
     ]
    }
   ],
   "source": [
    "# Select stocks based on ticker symbol\n",
    "stock_names = [\"A\", \"AAL\", \"AAP\", \"AAPEX\", \"AAPH\", \"AAPIX\", \"AAPJ\", \"AAPL\", \"ABBV\", \"ABEV\", \"ACB\", \"ACN\", \"ADBE\", \"ADIL\", \"ADM\", \"ADPT\", \"ADS\", \"AEE\", \"AEP\", \"AES\", \"AETI\", \"AFL\", \"AGNC\", \"AIG\", \"AIV\", \"AIZ\", \"AJG\", \"AKAM\", \"ALB\", \"ALGN\", \"ALK\", \"ALKS\", \"ALL\", \"ALLE\", \"ALLY\", \"ALXN\", \"AMAT\", \"AMCR\", \"AME\", \"AMG\", \"AMGN\", \"AMP\", \"AMRH\", \"AMRN\", \"AMT\", \"AMZN\", \"ANET\", \"ANSS\", \"ANTM\", \"ANY\", \"AON\", \"AOS\", \"APA\", \"APC\", \"APD\", \"APH\", \"APTV\", \"AQST\", \"ARCE\", \"ARE\", \"ARLO\", \"ARNC\", \"ARVN\", \"ATO\", \"ATVI\", \"AUY\", \"AVB\", \"AVGR\", \"AVTR\", \"AVY\", \"AXP\", \"AXSM\", \"AZO\", \"BA\", \"BABA\", \"BAC\", \"BAX\", \"BBY\", \"BEN\", \"BHGE\", \"BKNG\", \"BLL\", \"BLRX\", \"BMY\", \"BRK.B\", \"BSQR\", \"BSX\", \"BTAI\", \"BWA\", \"CAG\", \"CAH\", \"CAT\", \"CB\", \"CBRE\", \"CCI\", \"CDNS\", \"CELG\", \"CERN\", \"CGA\", \"CHRW\", \"CI\", \"CINF\", \"CL\", \"CLIR\", \"CLX\", \"CMA\", \"CMCSA\", \"CME\", \"CMG\", \"CMI\", \"CNC\", \"CNP\", \"COF\", \"COG\", \"COO\", \"COST\", \"COTY\", \"COUP\", \"CPB\", \"CPRI\", \"CPSH\", \"CSX\", \"CTAS\", \"CTK\", \"CTL\", \"CTRV\", \"CTSH\", \"CTVA\", \"CTXS\", \"CVX\", \"CX\", \"CXO\", \"D\", \"DAL\", \"DD\", \"DE\", \"DFS\", \"DHR\", \"DISCA\", \"DISCK\", \"DISH\", \"DLR\", \"DLTR\", \"DRE\", \"DRI\", \"DUK\", \"DVA\", \"DVN\", \"DXC\", \"EA\", \"EBAY\", \"ECA\", \"ECL\", \"ED\", \"EFX\", \"EMN\", \"EMR\", \"EOG\", \"EPD\", \"EQIX\", \"ES\", \"ESS\", \"ET\", \"ETN\", \"ETR\", \"EVRG\", \"EW\", \"EXC\", \"EXPD\", \"F\", \"FANG\", \"FARO\", \"FAST\", \"FB\", \"FBHS\", \"FCX\", \"FDC\", \"FDS\", \"FDX\", \"FE\", \"FFIV\", \"FIS\", \"FITB\", \"FL\", \"FLIR\", \"FLKS\", \"FLS\", \"FLT\", \"FMC\", \"FOX\", \"FOXA\", \"FRAN\", \"FRC\", \"FRT\", \"FTI\", \"FTNT\", \"FTV\", \"GE\", \"GFI\", \"GGB\", \"GILD\", \"GIS\", \"GLW\", \"GM\", \"GNMX\", \"GOLD\", \"GOOG\", \"GOOGL\", \"GPC\", \"GPK\", \"GPS\", \"GRMN\", \"GWW\", \"HAL\", \"HAS\", \"HBAN\", \"HBI\", \"HCA\", \"HCP\", \"HD\", \"HFC\", \"HII\", \"HLT\", \"HMC\", \"HOG\", \"HOV\", \"HP\", \"HPE\", \"HPQ\", \"HSGX\", \"HST\", \"HSY\", \"HUM\", \"IBM\", \"ICE\", \"IDXX\", \"IFF\", \"IGLD\", \"ILMN\", \"INCY\", \"INFO\", \"INFY\", \"INOD\", \"INTC\", \"IP\", \"IPG\", \"IPGP\", \"IR\", \"IRM\", \"ISRG\", \"IT\", \"ITUB\", \"ITW\", \"IVZ\", \"JD\", \"JEC\", \"JEF\", \"JKHY\", \"JNJ\", \"JNPR\", \"JPM\", \"JT\", \"JWN\", \"K\", \"KEY\", \"KEYS\", \"KGC\", \"KHC\", \"KIM\", \"KLAC\", \"KMB\", \"KMI\", \"KMX\", \"KO\", \"KOSS\", \"KR\", \"KSS\", \"KSU\", \"LEG\", \"LEN\", \"LHX\", \"LIN\", \"LKQ\", \"LMT\", \"LNT\", \"LOW\", \"LPTH\", \"LRCX\", \"LUV\", \"LYB\", \"M\", \"MA\", \"MAA\", \"MAC\", \"MAS\", \"MCHP\", \"MCK\", \"MCO\", \"MDLZ\", \"MDT\", \"MET\", \"MHK\", \"MKC\", \"MKTX\", \"MLM\", \"MMC\", \"MMM\", \"MNST\", \"MO\", \"MOS\", \"MPC\", \"MRK\", \"MRVL\", \"MS\", \"MSCI\", \"MTB\", \"MU\", \"MXIM\", \"MYSZ\", \"NBL\", \"NCLH\", \"NDAQ\", \"NEE\", \"NFLX\", \"NI\", \"NIO\", \"NKE\", \"NKTR\", \"NOC\", \"NOK\", \"NOV\", \"NRG\", \"NTAP\", \"NTNX\", \"NUE\", \"NVDA\", \"NVFY\", \"NWL\", \"NWS\", \"NWSA\", \"NYCB\", \"O\", \"OKE\", \"OMC\", \"ORCL\", \"ORLY\", \"OSS\", \"OXBR\", \"OXY\", \"PAYX\", \"PBCT\", \"PDD\", \"PEG\", \"PEP\", \"PFE\", \"PG\", \"PGR\", \"PH\", \"PHM\", \"PIR\", \"PKI\", \"PLAN\", \"PNC\", \"PNR\", \"PPG\", \"PPL\", \"PRGO\", \"PRU\", \"PSA\", \"PSX\", \"PVH\", \"PXD\", \"PYPL\", \"QBAK\", \"QRVO\", \"QUIK\", \"RBCN\", \"RCL\", \"RE\", \"REG\", \"REGN\", \"RHI\", \"RHT\", \"RIG\", \"RL\", \"RMD\", \"ROK\", \"ROKU\", \"ROL\", \"ROP\", \"ROST\", \"S\", \"SAN\", \"SBAC\", \"SBUX\", \"SCHW\", \"SEE\", \"SESN\", \"SFUN\", \"SGMA\", \"SHW\", \"SIRI\", \"SJM\", \"SLB\", \"SLG\", \"SMAR\", \"SNAP\", \"SNPS\", \"SO\", \"SPGI\", \"SRE\", \"STT\", \"STX\", \"STZ\", \"SWK\", \"SWKS\", \"SYK\", \"SYMC\", \"SYY\", \"TAP\", \"TDG\", \"TEL\", \"TEVA\", \"TFX\", \"TGT\", \"TIF\", \"TJX\", \"TME\", \"TMK\", \"TMO\", \"TMUS\", \"TNK\", \"TOPS\", \"TRHC\", \"TRIL\", \"TRIP\", \"TROW\", \"TRQ\", \"TRV\", \"TSLA\", \"TSM\", \"TSN\", \"TSS\", \"TTWO\", \"TWMC\", \"TWTR\", \"TXT\", \"UAA\", \"UAL\", \"UBER\", \"UDR\", \"UHS\", \"ULTA\", \"UNM\", \"UNP\", \"UPL\", \"UPS\", \"URI\", \"USB\", \"V\", \"VALE\", \"VEON\", \"VFC\", \"VIAB\", \"VIPS\", \"VLO\", \"VMC\", \"VRSK\", \"VRTX\", \"VTVT\", \"VZ\", \"WAT\", \"WBA\", \"WCG\", \"WDC\", \"WELL\", \"WFC\", \"WHR\", \"WLTW\", \"WM\", \"WMB\", \"WMT\", \"WPX\", \"WRK\", \"WU\", \"WY\", \"WYNN\", \"XEC\", \"XEL\", \"XLNX\", \"XOM\", \"XRAY\", \"XRX\", \"XYL\", \"YUM\", \"ZBH\", \"ZION\", \"ZNGA\", \"ZTS\"]\n",
    "print(len(stock_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the stock is valid\n",
    "\n",
    "# import requests\n",
    "# import re\n",
    "# import time\n",
    "\n",
    "# target_string = \"{\\\"GlobalQuote\\\":{}}\"\n",
    "# _bad_string = \"{\\\"Note\\\": \\\"Thank you for using Alpha Vantage! Our standard API call frequency is 5 calls per minute and 500 calls per day. Please visit https://www.alphavantage.co/premium/ if you would like to target a higher API call frequency.\\\"}\"\n",
    "# bad_string = re.sub(r\"\\s\", \"\", _bad_string)\n",
    "# count = 0\n",
    "# true_count = 0\n",
    "# true_length = len(stock_names) / 5\n",
    "\n",
    "# for stock in stock_names:\n",
    "#     _result = requests.get(\"https://www.alphavantage.co/query?function=GLOBAL_QUOTE&symbol=\" + stock + \"&apikey=SSAQZVKJHSN826NT\").text\n",
    "#     result = re.sub(r\"\\s\", \"\", _result)\n",
    "\n",
    "#     if (result == bad_string):\n",
    "#         print(\"overtime\")\n",
    "#         time.sleep(60)\n",
    "#         _result = requests.get(\"https://www.alphavantage.co/query?function=GLOBAL_QUOTE&symbol=\" + stock + \"&apikey=SSAQZVKJHSN826NT\").text\n",
    "#         result = re.sub(r\"\\s\", \"\", _result)\n",
    "    \n",
    "#     if (result == target_string):\n",
    "#         print(stock, result)\n",
    "    \n",
    "#     count += 1\n",
    "#     if (count == 5):\n",
    "#         count = 0\n",
    "#         true_count += 1\n",
    "#         print(\"progress: \", true_count, \"/\", true_length)\n",
    "#         time.sleep(61)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total ( 499 )\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Time Series (Daily)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8dda452d5c1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Store Stock information as a data set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstock_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_get_daily\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-08cf15391f1f>\u001b[0m in \u001b[0;36mbatch_get_daily\u001b[0;34m(stock_list)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m65\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 65 seconds just in case the timing on the server or here isn't perfect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_daily\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mbig_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\":\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbig_counter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-08cf15391f1f>\u001b[0m in \u001b[0;36mget_daily\u001b[0;34m(stock)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_daily\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimeSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAPIkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pandas'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_daily\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'full'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/alpha_vantage/alphavantage.py\u001b[0m in \u001b[0;36m_format_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'json'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'pandas'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_response\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmeta_data_key\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                     \u001b[0mmeta_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_response\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmeta_data_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Time Series (Daily)'"
     ]
    }
   ],
   "source": [
    "# Store Stock information as a data set\n",
    "stock_data = batch_get_daily(stock_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print(stock_data[0][0])\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f = open('store_stocks_new.pckl', 'wb') # rename and remove '_new' to load it\n",
    "pickle.dump(stock_data, f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open('store_stocks.pckl', 'rb')\n",
    "stock_data = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "print(len(stock_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Format dataset with classification (up/down)\n",
    "import numpy as np\n",
    "\n",
    "def add_classification(stocks):\n",
    "    # up = 1, down = 0\n",
    "    new_stocks = []\n",
    "    # column 4 is the end of day price\n",
    "    last_price = stocks[len(stocks) - 1][4]\n",
    "    # the very last price will always be up. Perhaps\n",
    "    # this makes my AI optimistic?\n",
    "    new_stocks.append(np.insert(stocks[len(stocks) - 1], 5, 1)) # up\n",
    "    for s in reversed(list(range(len(stocks) - 1))):\n",
    "        if (stocks[s][4] - last_price) > 0:\n",
    "            new_stocks.append(np.insert(stocks[s], 5, 1)) # up\n",
    "        else:\n",
    "            new_stocks.append(np.insert(stocks[s], 5, 0)) # down\n",
    "        last_price = stocks[s][4]\n",
    "    return new_stocks\n",
    "\n",
    "# accepts an array of all data from one stock\n",
    "# produces chunks to be made into tensors\n",
    "# run this once on each stock and save the resulting array (of arrays)\n",
    "def split_sample(data):\n",
    "    sample = []\n",
    "    for start in range(0, len(data) - 50, 50):\n",
    "        sample.append(data[start:start+50])\n",
    "    return sample\n",
    "\n",
    "# takes the results of batch_get_daily and prepares everything\n",
    "def prepare(group_of_stocks):\n",
    "    prepared_data_set = []\n",
    "    for stock in group_of_stocks:\n",
    "        prepared_data_set.append(split_sample(add_classification(stock[0].to_numpy())))\n",
    "    return prepared_data_set\n",
    "\n",
    "def prep_part_two(group_of_stocks):\n",
    "    new_set = []\n",
    "    for stock in group_of_stocks:\n",
    "        for collection in stock:\n",
    "            classification = collection[0][5]\n",
    "            training_data = collection[1:]\n",
    "            new_set.append([training_data, classification])\n",
    "    return new_set\n",
    "\n",
    "# prepped_stock[n][m][0][5] = the classification!\n",
    "# prepped_stock[n][m][X > 0] = the data sets!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# an example of how to use this data and functions\n",
    "# print(split_sample(add_classification(stock_data[0][0].to_numpy()))[0])\n",
    "\n",
    "# results of preparing the stocks\n",
    "prepped_stocks = prepare(stock_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# pickle the prepped stocks!\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f = open('store_prepped_stocks_new.pckl', 'wb') # rename and remove '_new' to load it\n",
    "pickle.dump(prepped_stocks, f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f = open('store_prepped_stocks.pckl', 'rb')\n",
    "prepped_stocks = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "print(len(prepped_stocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# prepped stocks part 2\n",
    "data_set = prep_part_two(prepped_stocks)\n",
    "\n",
    "# now the data should be one array filled with pairs of 49 days worth of \n",
    "# data with the classification of the next day.\n",
    "\n",
    "# TODO: so I need to split this into x_train, y_train and x_test, y_test\n",
    "\n",
    "print(len(data_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# save my work\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f = open('store_data_set_new.pckl', 'wb') # rename and remove '_new' to load it\n",
    "pickle.dump(data_set, f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f = open('store_data_set.pckl', 'rb')\n",
    "data_set = pickle.load(f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# x_train, y_train, x_test, y_test\n",
    "train_data = []\n",
    "train_class = []\n",
    "test_data = []\n",
    "test_class = []\n",
    "\n",
    "np.random.shuffle(data_set)\n",
    "\n",
    "# print(data_set[:4])\n",
    "\n",
    "for combo in data_set:\n",
    "    train_data.append(combo[0])\n",
    "    train_class.append(combo[1])\n",
    "\n",
    "test_data = train_data[7707:9707]\n",
    "test_class = train_class[7707:9707]\n",
    "train_data = train_data[0:7706]\n",
    "train_class = train_class[0:7706]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# save my work\n",
    "import pickle\n",
    "# train_data = []\n",
    "# train_class = []\n",
    "# test_data = []\n",
    "# test_class = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f = open('store_train_data_new.pckl', 'wb') # rename and remove '_new' to load it\n",
    "pickle.dump(train_data, f)\n",
    "f.close()\n",
    "\n",
    "f = open('store_train_class_new.pckl', 'wb') # rename and remove '_new' to load it\n",
    "pickle.dump(train_class, f)\n",
    "f.close()\n",
    "\n",
    "f = open('store_test_data_new.pckl', 'wb') # rename and remove '_new' to load it\n",
    "pickle.dump(test_data, f)\n",
    "f.close()\n",
    "\n",
    "f = open('store_test_class_new.pckl', 'wb') # rename and remove '_new' to load it\n",
    "pickle.dump(test_class, f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f = open('store_train_data.pckl', 'rb')\n",
    "train_data = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('store_train_class.pckl', 'rb')\n",
    "train_class = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('store_test_data.pckl', 'rb')\n",
    "test_data = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('store_test_class.pckl', 'rb')\n",
    "test_class = pickle.load(f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# (3 must-haves in one cell)\n",
    "# Train\n",
    "# Hidden Nodes\n",
    "# Dropout\n",
    "import tensorflow as tf\n",
    "\n",
    "# example training\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(49, 6)),\n",
    "    tf.keras.layers.Dense(512, activation=tf.nn.softmax),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(2, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(np.array(train_data), np.array(train_class), epochs=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "model.evaluate(np.array(test_data), np.array(test_class))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Real Life Prediction\n",
    "def get_daily_compact(stock):\n",
    "    ts = TimeSeries(key=APIkey, output_format='pandas')\n",
    "    data, meta_data = ts.get_daily(symbol=stock, outputsize='compact') # this returns 100 datapoints\n",
    "    return data\n",
    "\n",
    "chosen_stock = \"AAPL\"\n",
    "rlp_stock = get_daily_compact(chosen_stock)\n",
    "rlp_data = split_sample(add_classification(rlp_stock.to_numpy()))[0]\n",
    "rlp_data = [rlp_data[:49]]\n",
    "\n",
    "prediction = model.predict_classes(np.array(rlp_data), batch_size=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Print results of prediction\n",
    "\n",
    "# 1 = tomorrow will be up\n",
    "# 0 = tomorrow will be down\n",
    "\n",
    "if (prediction == 1):\n",
    "    print(chosen_stock + \" will go up tomorrow\")\n",
    "else:\n",
    "    print(chosen_stock + \" will go down tomorrow\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (verdurous-stock-market-ai)",
   "language": "python",
   "name": "pycharm-6f9556a3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
