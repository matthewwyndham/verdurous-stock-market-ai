{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Run me first to import and setup everything\n",
    "import time\n",
    "import csv  \n",
    "from myapikey import APIkey\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def add_classification(stocks):\n",
    "    new_stocks = []\n",
    "    last_price = stocks[len(stocks) - 1][4]\n",
    "    new_stocks.append(np.insert(stocks[len(stocks) - 1], 5, 1)) # up\n",
    "    for s in reversed(list(range(len(stocks) - 1))):\n",
    "        if (stocks[s][4] - last_price) > 0:\n",
    "            new_stocks.append(np.insert(stocks[s], 5, 1)) # up\n",
    "        else:\n",
    "            new_stocks.append(np.insert(stocks[s], 5, 0)) # down\n",
    "        last_price = stocks[s][4]\n",
    "    return new_stocks\n",
    "\n",
    "def split_sample(data):\n",
    "    sample = []\n",
    "    for start in range(0, len(data) - 50, 50):\n",
    "        sample.append(data[start:start+50])\n",
    "    return sample\n",
    "\n",
    "def prepare(group_of_stocks):\n",
    "    prepared_data_set = []\n",
    "    for stock in group_of_stocks:\n",
    "        prepared_data_set.append(split_sample(add_classification(stock[0].to_numpy())))\n",
    "    return prepared_data_set\n",
    "\n",
    "def prep_part_two(group_of_stocks):\n",
    "    new_set = []\n",
    "    for stock in group_of_stocks:\n",
    "        for collection in stock:\n",
    "            classification = collection[0][5]\n",
    "            training_data = collection[1:]\n",
    "            new_set.append([training_data, classification])\n",
    "    return new_set\n",
    "\n",
    "# def get_intraday(stock):\n",
    "#     ts = TimeSeries(key=APIkey, output_format='pandas', indexing_type='integer')\n",
    "#     return ts.get_intraday(symbol=stock, interval='1min', outputsize='full')\n",
    "\n",
    "def get_daily(stock):\n",
    "    ts = TimeSeries(key=APIkey, output_format='pandas')\n",
    "    data, meta_data = ts.get_daily(symbol=stock, outputsize='full')\n",
    "    return data, meta_data\n",
    "\n",
    "def open_csv(path_to_csv):\n",
    "    data = []\n",
    "    with open(path_to_csv) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            if row[0] != '':\n",
    "                data.append(row)\n",
    "    return data\n",
    "\n",
    "def batch_get_daily(stock_list):\n",
    "    count = 0\n",
    "    data = []\n",
    "    for stock in stock_list:\n",
    "        if count == 5:\n",
    "            time.sleep(65)\n",
    "            count = 0\n",
    "        data.append(get_daily(stock))\n",
    "        count += 1\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "#######                           ########\n",
    "#######    WOULD LIKE TO HAVES    ########\n",
    "#######                           ########\n",
    "##########################################\n",
    "\n",
    "print(\"would like to haves\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save data as CSV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Wait for API (free) access limit\n",
    "import time\n",
    "\n",
    "def batch_get_daily(stock_list):\n",
    "    count = 0\n",
    "    data = []\n",
    "    for stock in stock_list:\n",
    "        if count == 5:\n",
    "            time.sleep(65) # 65 seconds just in case the timing on the server or here isn't perfect\n",
    "            count = 0\n",
    "        data.append(get_daily(stock))\n",
    "        count += 1\n",
    "    return data\n",
    "\n",
    "# for 130 stocks this should take at least half an hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save results of Training\n",
    "# https://www.tensorflow.org/guide/keras#entire_model\n",
    "model.save('verdurouMKI.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.engine.sequential.Sequential object at 0x7f7cb00c07f0>\n"
     ]
    }
   ],
   "source": [
    "# Load Results from Previous Training\n",
    "loaded_model = tf.keras.models.load_model('verdurouMKI.h5')\n",
    "print(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Scrape HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Must Haves\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "############                  ############\n",
    "############    MUST HAVES    ############\n",
    "############                  ############\n",
    "##########################################\n",
    "\n",
    "print(\"Must Haves\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Read CSV\n",
    "# This will read in the CSVs that come from the alpha vantage website, where each CSV\n",
    "# is from a separate stock symbol\n",
    "import csv  \n",
    "\n",
    "def open_csv(path_to_csv):\n",
    "    data = []\n",
    "    with open(path_to_csv) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            if row[0] != '':\n",
    "                data.append(row)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Access API\n",
    "from myapikey import APIkey\n",
    "from alpha_vantage.timeseries import TimeSeries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# must provide an api key (they are free from the alpha vantage website\n",
    "# this determines how much data you can get in how much time.\n",
    "def get_intraday(stock):\n",
    "    ts = TimeSeries(key=APIkey, output_format='pandas', indexing_type='integer')\n",
    "    return ts.get_intraday(symbol=stock, interval='1min', outputsize='full')\n",
    "\n",
    "# provide an api key to access the data\n",
    "def get_daily(stock):\n",
    "    ts = TimeSeries(key=APIkey, output_format='pandas')\n",
    "    data, meta_data = ts.get_daily(symbol=stock, outputsize='full')\n",
    "    return data, meta_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Select stocks based on ticker symbol\n",
    "stock_names = [\"AAOI\", \"AAP\", \"AAPL\", \"ABMD\", \"ABT\", \"ACET\", \"ADIL\", \"AETI\", \"AFL\", \"AKAM\", \"ALGN\", \"ALQA\", \"AMD\", \n",
    "\"AMG\", \"AMRH\", \"AMZN\", \"ANY\", \"APVO\", \"AVGR\", \"AXSM\", \"BAC\", \"BIIB\", \"BLIN\", \"BLK\", \"BLRX\", \"BSQR\", \"BSX\", \"BTAI\", \n",
    "\"CGA\", \"CGC\", \"CLIR\", \"CMCSA\", \"CMG\", \"COP\", \"COTY\", \"COUP\", \"CPSH\", \"CSCO\", \"CTK\", \"CTRV\", \"CVX\", \"D\", \"DIS\", \n",
    "\"DUK\", \"EXC\", \"EYES\", \"FARO\", \"FB\", \"FCEL\", \"FDS\", \"FLKS\", \"FRAN\", \"FTNT\", \"GE\", \"GM\", \"GNMX\", \"GOOGL\", \"HMC\", \"HOV\", \n",
    "\"HSGX\", \"IGLD\", \"INOD\", \"IVZ\", \"JAGX\", \"JNJ\", \"JPM\", \"JT\", \"KEYS\", \"KOSS\", \"LB\", \"LLY\", \"LPTH\", \"M\", \"MDT\", \"MSFT\", \n",
    "\"MYSZ\", \"NDAQ\", \"NEE\", \"NFLX\", \"NKE\", \"NRG\", \"NVDA\", \"NVFY\", \"ORLY\", \"OSS\", \"OXBR\", \"PFE\", \"PIR\", \"PRGO\", \"QBAK\", \n",
    "\"QUIK\", \"RAD\", \"RBCN\",\"RHT\", \"RL\", \"SBUX\", \"SESN\", \"SFUN\", \"SGMA\", \"SMAR\", \"SO\", \"STZ\", \"SYK\", \"T\", \"TNK\", \"TOPS\", \n",
    "\"TRHC\", \"TRIL\", \"TRIP\", \"TRNX\", \"TSLA\", \"TSN\", \"TTWO\", \"TWMC\", \"TWTR\", \"UA\", \"UAA\", \"UPL\", \"USB\", \"UXIN\", \"V\", \n",
    "\"VRSN\", \"VTVT\", \"VZ\", \"WDC\", \"WFC\", \"WFT\", \"WMT\", \"XOM\", \"ZTS\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Store Stock information as a data set\n",
    "stock_data = batch_get_daily(stock_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print(stock_data[0][0])\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f = open('store_stocks_new.pckl', 'wb') # rename and remove '_new' to load it\n",
    "pickle.dump(stock_data, f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n"
     ]
    }
   ],
   "source": [
    "f = open('store_stocks.pckl', 'rb')\n",
    "stock_data = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "print(len(stock_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Format dataset with classification (up/down)\n",
    "import numpy as np\n",
    "\n",
    "def add_classification(stocks):\n",
    "    # up = 1, down = 0\n",
    "    new_stocks = []\n",
    "    # column 4 is the end of day price\n",
    "    last_price = stocks[len(stocks) - 1][4]\n",
    "    # the very last price will always be up. Perhaps\n",
    "    # this makes my AI optimistic?\n",
    "    new_stocks.append(np.insert(stocks[len(stocks) - 1], 5, 1)) # up\n",
    "    for s in reversed(list(range(len(stocks) - 1))):\n",
    "        if (stocks[s][4] - last_price) > 0:\n",
    "            new_stocks.append(np.insert(stocks[s], 5, 1)) # up\n",
    "        else:\n",
    "            new_stocks.append(np.insert(stocks[s], 5, 0)) # down\n",
    "        last_price = stocks[s][4]\n",
    "    return new_stocks\n",
    "\n",
    "# accepts an array of all data from one stock\n",
    "# produces chunks to be made into tensors\n",
    "# run this once on each stock and save the resulting array (of arrays)\n",
    "def split_sample(data):\n",
    "    sample = []\n",
    "    for start in range(0, len(data) - 50, 50):\n",
    "        sample.append(data[start:start+50])\n",
    "    return sample\n",
    "\n",
    "# takes the results of batch_get_daily and prepares everything\n",
    "def prepare(group_of_stocks):\n",
    "    prepared_data_set = []\n",
    "    for stock in group_of_stocks:\n",
    "        prepared_data_set.append(split_sample(add_classification(stock[0].to_numpy())))\n",
    "    return prepared_data_set\n",
    "\n",
    "def prep_part_two(group_of_stocks):\n",
    "    new_set = []\n",
    "    for stock in group_of_stocks:\n",
    "        for collection in stock:\n",
    "            classification = collection[0][5]\n",
    "            training_data = collection[1:]\n",
    "            new_set.append([training_data, classification])\n",
    "    return new_set\n",
    "\n",
    "# prepped_stock[n][m][0][5] = the classification!\n",
    "# prepped_stock[n][m][X > 0] = the data sets!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# an example of how to use this data and functions\n",
    "# print(split_sample(add_classification(stock_data[0][0].to_numpy()))[0])\n",
    "\n",
    "# results of preparing the stocks\n",
    "prepped_stocks = prepare(stock_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# pickle the prepped stocks!\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f = open('store_prepped_stocks_new.pckl', 'wb') # rename and remove '_new' to load it\n",
    "pickle.dump(prepped_stocks, f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n"
     ]
    }
   ],
   "source": [
    "f = open('store_prepped_stocks.pckl', 'rb')\n",
    "prepped_stocks = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "print(len(prepped_stocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "# print(len(prepped_stocks))\n",
    "# 130\n",
    "# print(len(prepped_stocks[0]))\n",
    "# 28\n",
    "# print(len(prepped_stocks[0][0]))\n",
    "# 50\n",
    "# print(len(prepped_stocks[0][0][0]))\n",
    "# 6\n",
    "\n",
    "# print(len(prepped_stocks[3]))\n",
    "# 107\n",
    "\n",
    "# each stock contains a (small to large) number of sets of 50 lines of info about a stock\n",
    "# each line contains 6 data points, with the final data point being 1 or 0\n",
    "# where 1 is up from the previous day and 0 is down from the previous day\n",
    "\n",
    "# prepped_stock[n][m][0][5] = the classification!\n",
    "# prepped_stock[n][m][X > 0] = the data sets!\n",
    "\n",
    "# I want to make a list of each of the 50 training bits, but I want only 49 of the bits\n",
    "# so the 1st bit will be the classification. The 50 training bits should actually be \n",
    "# 49*6 bits of info in one array. Then the second member of the array containing that data\n",
    "# will be 0 or 1. Then it will be a list of everything!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9708\n"
     ]
    }
   ],
   "source": [
    "# prepped stocks part 2\n",
    "data_set = prep_part_two(prepped_stocks)\n",
    "\n",
    "# now the data should be one array filled with pairs of 49 days worth of \n",
    "# data with the classification of the next day.\n",
    "\n",
    "# TODO: so I need to split this into x_train, y_train and x_test, y_test\n",
    "\n",
    "print(len(data_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# save my work\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f = open('store_data_set_new.pckl', 'wb') # rename and remove '_new' to load it\n",
    "pickle.dump(data_set, f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f = open('store_data_set.pckl', 'rb')\n",
    "data_set = pickle.load(f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# x_train, y_train, x_test, y_test\n",
    "train_data = []\n",
    "train_class = []\n",
    "test_data = []\n",
    "test_class = []\n",
    "\n",
    "np.random.shuffle(data_set)\n",
    "\n",
    "# print(data_set[:4])\n",
    "\n",
    "for combo in data_set:\n",
    "    train_data.append(combo[0])\n",
    "    train_class.append(combo[1])\n",
    "\n",
    "test_data = train_data[7707:9707]\n",
    "test_class = train_class[7707:9707]\n",
    "train_data = train_data[0:7706]\n",
    "train_class = train_class[0:7706]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-d7906cd0f670>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mone\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msmall_data_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtwo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mthree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtwo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfour\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthree\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfour\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxNumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not iterable"
     ]
    }
   ],
   "source": [
    "normalized_data_set = []\n",
    "maxNumber = 0.0\n",
    "# small_data_set = data_set[:10]\n",
    "for one in small_data_set:\n",
    "    for two in one:\n",
    "        for three in two:\n",
    "            for four in three:\n",
    "                if (four > maxNumber):\n",
    "                    maxNumber = 0\n",
    "print(maxNumber)\n",
    "# print(small_data_set[0][0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.21300e+01 8.10000e+01 7.20000e+01 8.00000e+01 1.14058e+07 1.00000e+00]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(train_data[5][1])\n",
    "print(train_class[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# save my work\n",
    "import pickle\n",
    "# train_data = []\n",
    "# train_class = []\n",
    "# test_data = []\n",
    "# test_class = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f = open('store_train_data_new.pckl', 'wb') # rename and remove '_new' to load it\n",
    "pickle.dump(train_data, f)\n",
    "f.close()\n",
    "\n",
    "f = open('store_train_class_new.pckl', 'wb') # rename and remove '_new' to load it\n",
    "pickle.dump(train_class, f)\n",
    "f.close()\n",
    "\n",
    "f = open('store_test_data_new.pckl', 'wb') # rename and remove '_new' to load it\n",
    "pickle.dump(test_data, f)\n",
    "f.close()\n",
    "\n",
    "f = open('store_test_class_new.pckl', 'wb') # rename and remove '_new' to load it\n",
    "pickle.dump(test_class, f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f = open('store_train_data.pckl', 'rb')\n",
    "train_data = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('store_train_class.pckl', 'rb')\n",
    "train_class = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('store_test_data.pckl', 'rb')\n",
    "test_data = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('store_test_class.pckl', 'rb')\n",
    "test_class = pickle.load(f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "7706/7706 [==============================] - 2s 293us/step - loss: 8.7650 - acc: 0.4561\n",
      "Epoch 2/7\n",
      "7706/7706 [==============================] - 1s 183us/step - loss: 8.7744 - acc: 0.4556\n",
      "Epoch 3/7\n",
      "7706/7706 [==============================] - 1s 177us/step - loss: 8.5298 - acc: 0.4707\n",
      "Epoch 4/7\n",
      "7706/7706 [==============================] - 1s 177us/step - loss: 7.2914 - acc: 0.5476\n",
      "Epoch 5/7\n",
      "7706/7706 [==============================] - 1s 177us/step - loss: 7.2872 - acc: 0.5479\n",
      "Epoch 6/7\n",
      "7706/7706 [==============================] - 1s 177us/step - loss: 7.2935 - acc: 0.5475\n",
      "Epoch 7/7\n",
      "7706/7706 [==============================] - 1s 174us/step - loss: 7.2914 - acc: 0.5476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7ca776ccc0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (3 must-haves in one cell)\n",
    "# Train\n",
    "# Hidden Nodes\n",
    "# Dropout\n",
    "import tensorflow as tf\n",
    "\n",
    "# example training\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(49, 6)),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.3),\n",
    "  tf.keras.layers.Dense(2, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(np.array(train_data), np.array(train_class), epochs=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 55us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[8.961661651611328, 0.444]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "model.evaluate(np.array(test_data), np.array(test_class))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Real Life Prediction\n",
    "def get_daily_compact(stock):\n",
    "    ts = TimeSeries(key=APIkey, output_format='pandas')\n",
    "    data, meta_data = ts.get_daily(symbol=stock, outputsize='compact') # this returns 100 datapoints\n",
    "    return data\n",
    "\n",
    "chosen_stock = \"AAPL\"\n",
    "rlp_stock = get_daily_compact(chosen_stock)\n",
    "rlp_data = split_sample(add_classification(rlp_stock.to_numpy()))[0]\n",
    "rlp_data = [rlp_data[1:]]\n",
    "\n",
    "# for testing (internet is down)\n",
    "# rlp_stock = stock_data[0][0]\n",
    "# rlp_data = split_sample(add_classification(rlp_stock.to_numpy()))[0]\n",
    "# print(rlp_data[0][5]) # the actual result\n",
    "# rlp_data = [rlp_data[1:]]\n",
    "\n",
    "prediction = model.predict_classes(np.array(rlp_data), batch_size=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL will go up tomorrow\n"
     ]
    }
   ],
   "source": [
    "# Print results of prediction\n",
    "\n",
    "# 1 = tomorrow will be up\n",
    "# 0 = tomorrow will be down\n",
    "\n",
    "if (prediction == 1):\n",
    "    print(chosen_stock + \" will go up tomorrow\")\n",
    "else:\n",
    "    print(chosen_stock + \" will go down tomorrow\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (verdurous-stock-market-ai)",
   "language": "python",
   "name": "pycharm-6f9556a3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
