{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Must Haves\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "##########################################\n",
    "############                  ############\n",
    "############    MUST HAVES    ############\n",
    "############                  ############\n",
    "##########################################\n",
    "\n",
    "print(\"Must Haves\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Read CSV\n",
    "# This will read in the CSVs that come from the alpha vantage website, where each CSV\n",
    "# is from a separate stock symbol\n",
    "import csv  \n",
    "\n",
    "def open_csv(path_to_csv):\n",
    "    data = []\n",
    "    with open(path_to_csv) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            if row[0] != '':\n",
    "                data.append(row)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Access API\n",
    "from myapikey import APIkey\n",
    "from alpha_vantage.timeseries import TimeSeries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# must provide an api key (they are free from the alpha vantage website\n",
    "# this determines how much data you can get in how much time.\n",
    "def get_intraday(stock):\n",
    "    ts = TimeSeries(key=APIkey, output_format='pandas', indexing_type='integer')\n",
    "    return ts.get_intraday(symbol=stock, interval='1min', outputsize='full')\n",
    "\n",
    "# provide an api key to access the data\n",
    "def get_daily(stock):\n",
    "    ts = TimeSeries(key=APIkey, output_format='pandas')\n",
    "    data, meta_data = ts.get_daily(symbol=stock, outputsize='full')\n",
    "    return data, meta_data\n"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "130\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Select stocks based on ticker symbol\n",
    "stock_names = [\"AAOI\", \"AAP\", \"AAPL\", \"ABMD\", \"ABT\", \"ACET\", \"ADIL\", \"AETI\", \"AFL\", \"AKAM\", \"ALGN\", \"ALQA\", \"AMD\", \n",
    "\"AMG\", \"AMRH\", \"AMZN\", \"ANY\", \"APVO\", \"AVGR\", \"AXSM\", \"BAC\", \"BIIB\", \"BLIN\", \"BLK\", \"BLRX\", \"BSQR\", \"BSX\", \"BTAI\", \n",
    "\"CGA\", \"CGC\", \"CLIR\", \"CMCSA\", \"CMG\", \"COP\", \"COTY\", \"COUP\", \"CPSH\", \"CSCO\", \"CTK\", \"CTRV\", \"CVX\", \"D\", \"DIS\", \n",
    "\"DUK\", \"EXC\", \"EYES\", \"FARO\", \"FB\", \"FCEL\", \"FDS\", \"FLKS\", \"FRAN\", \"FTNT\", \"GE\", \"GM\", \"GNMX\", \"GOOGL\", \"HMC\", \"HOV\", \n",
    "\"HSGX\", \"IGLD\", \"INOD\", \"IVZ\", \"JAGX\", \"JNJ\", \"JPM\", \"JT\", \"KEYS\", \"KOSS\", \"LB\", \"LLY\", \"LPTH\", \"M\", \"MDT\", \"MSFT\", \n",
    "\"MYSZ\", \"NDAQ\", \"NEE\", \"NFLX\", \"NKE\", \"NRG\", \"NVDA\", \"NVFY\", \"ORLY\", \"OSS\", \"OXBR\", \"PFE\", \"PIR\", \"PRGO\", \"QBAK\", \n",
    "\"QUIK\", \"RAD\", \"RBCN\",\"RHT\", \"RL\", \"SBUX\", \"SESN\", \"SFUN\", \"SGMA\", \"SMAR\", \"SO\", \"STZ\", \"SYK\", \"T\", \"TNK\", \"TOPS\", \n",
    "\"TRHC\", \"TRIL\", \"TRIP\", \"TRNX\", \"TSLA\", \"TSN\", \"TTWO\", \"TWMC\", \"TWTR\", \"UA\", \"UAA\", \"UPL\", \"USB\", \"UXIN\", \"V\", \n",
    "\"VRSN\", \"VTVT\", \"VZ\", \"WDC\", \"WFC\", \"WFT\", \"WMT\", \"XOM\", \"ZTS\"]\n",
    "\n",
    "# at least one of these is not a real stock recognized by alpha vantage.\n",
    "# I must error check these stocks.\n",
    "print(len(stock_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Store Stock information as a data set\n",
    "stock_data = batch_get_daily(stock_names)\n"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# print(stock_data[0][0])\n",
    "import pickle\n"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "f = open('store_stocks_new.pckl', 'wb') # rename and remove '_new' to load it\n",
    "pickle.dump(stock_data, f)\n",
    "f.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "f = open('store_stocks.pckl', 'rb')\n",
    "stock_data = pickle.load(f)\n",
    "f.close()\n"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Format dataset with classification (up/down)\n",
    "import numpy as np\n",
    "\n",
    "def add_classification(stocks):\n",
    "    # up = 1, down = 0\n",
    "    new_stocks = []\n",
    "    # column 4 is the end of day price\n",
    "    last_price = stocks[len(stocks) - 1][4]\n",
    "    # the very last price will always be up. Perhaps\n",
    "    # this makes my AI optimistic?\n",
    "    new_stocks.append(np.insert(stocks[len(stocks) - 1], 5, 1)) # up\n",
    "    for s in reversed(list(range(len(stocks) - 1))):\n",
    "        if (stocks[s][4] - last_price) > 0:\n",
    "            new_stocks.append(np.insert(stocks[s], 5, 1)) # up\n",
    "        else:\n",
    "            new_stocks.append(np.insert(stocks[s], 5, 0)) # down\n",
    "        last_price = stocks[s][4]\n",
    "    return new_stocks\n",
    "\n",
    "# accepts an array of all data from one stock\n",
    "# produces chunks to be made into tensors\n",
    "# run this once on each stock and save the resulting array (of arrays)\n",
    "def split_sample(data):\n",
    "    sample = []\n",
    "    for start in range(0, len(data) - 50, 50):\n",
    "        sample.append(data[start:start+50])\n",
    "    return sample\n",
    "\n",
    "# takes the results of batch_get_daily and prepares everything\n",
    "def prepare(group_of_stocks):\n",
    "    prepared_data_set = []\n",
    "    for stock in group_of_stocks:\n",
    "        prepared_data_set.append(split_sample(add_classification(stock[0].to_numpy())))\n",
    "    return prepared_data_set\n",
    "\n",
    "def prep_part_two(group_of_stocks):\n",
    "    new_set = []\n",
    "    for stock in group_of_stocks:\n",
    "        for collection in stock:\n",
    "            classification = collection[0][5]\n",
    "            training_data = collection[1:]\n",
    "            new_set.append([training_data, classification])\n",
    "    return new_set\n",
    "\n",
    "# prepped_stock[n][m][0][5] = the classification!\n",
    "# prepped_stock[n][m][X > 0] = the data sets!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# an example of how to use this data and functions\n",
    "# print(split_sample(add_classification(stock_data[0][0].to_numpy()))[0])\n",
    "\n",
    "# results of preparing the stocks\n",
    "prepped_stocks = prepare(stock_data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# pickle the prepped stocks!\n",
    "import pickle\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "f = open('store_prepped_stocks_new.pckl', 'wb') # rename and remove '_new' to load it\n",
    "pickle.dump(prepped_stocks, f)\n",
    "f.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "130\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "f = open('store_prepped_stocks.pckl', 'rb')\n",
    "prepped_stocks = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "print(len(prepped_stocks))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "28\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# print(len(prepped_stocks))\n",
    "# 130\n",
    "# print(len(prepped_stocks[0]))\n",
    "# 28\n",
    "# print(len(prepped_stocks[0][0]))\n",
    "# 50\n",
    "# print(len(prepped_stocks[0][0][0]))\n",
    "# 6\n",
    "\n",
    "# print(len(prepped_stocks[3]))\n",
    "# 107\n",
    "\n",
    "# each stock contains a (small to large) number of sets of 50 lines of info about a stock\n",
    "# each line contains 6 data points, with the final data point being 1 or 0\n",
    "# where 1 is up from the previous day and 0 is down from the previous day\n",
    "\n",
    "# prepped_stock[n][m][0][5] = the classification!\n",
    "# prepped_stock[n][m][X > 0] = the data sets!\n",
    "\n",
    "# I want to make a list of each of the 50 training bits, but I want only 49 of the bits\n",
    "# so the 1st bit will be the classification. The 50 training bits should actually be \n",
    "# 49*6 bits of info in one array. Then the second member of the array containing that data\n",
    "# will be 0 or 1. Then it will be a list of everything!\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "9795\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# prepped stocks part 2\n",
    "data_set = prep_part_two(prepped_stocks)\n",
    "\n",
    "# now the data should be one array filled with pairs of 49 days worth of \n",
    "# data with the classification of the next day.\n",
    "\n",
    "# TODO: so I need to split this into x_train, y_train and x_test, y_test\n",
    "\n",
    "print(len(data_set))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save my work\n",
    "import pickle\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "f = open('store_data_set_new.pckl', 'wb') # rename and remove '_new' to load it\n",
    "pickle.dump(data_set, f)\n",
    "f.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f = open('store_data_set.pckl', 'rb')\n",
    "data_set = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "print(len(prepped_stocks))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-31ccf2a220fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m               metrics=['accuracy'])\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ],
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error"
    }
   ],
   "source": [
    "# (3 must-haves in one cell)\n",
    "# Train\n",
    "# Hidden Nodes\n",
    "# Dropout\n",
    "import tensorflow as tf\n",
    "\n",
    "# example training\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": "# Test\nmodel.evaluate(x_test, y_test)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": "# Real Life Prediction\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": "# Print results of prediction\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": "##########################################\n#######                           ########\n#######    WOULD LIKE TO HAVES    ########\n#######                           ########\n##########################################\n\nprint(\"would like to haves\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": "# Save data as CSV\n\n"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": "# Wait for API (free) access limit\nimport time\n\ndef batch_get_daily(stock_list):\n    count = 0\n    data = []\n    for stock in stock_list:\n        if count == 5:\n            time.sleep(65) # 65 seconds just in case the timing on the server or here isn't perfect\n            count = 0\n        data.append(get_daily(stock))\n        count += 1\n    return data\n\n# for 130 stocks this should take at least half an hour\n"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5a13f031c189>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save results of Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# https://www.tensorflow.org/guide/keras#entire_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'verdurouMKI.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ],
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error"
    }
   ],
   "source": "# Save results of Training\n# https://www.tensorflow.org/guide/keras#entire_model\nmodel.save_weights('verdurouMKI.h5')\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": "# Load Results from Previous Training\nmodel = tf.keras.models.load_model('verdurouMKI.h5')\n"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": "# Scrape HTML\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": "\n",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-6f9556a3",
   "language": "python",
   "display_name": "PyCharm (verdurous-stock-market-ai)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "stem_cell": {
   "cell_type": "raw",
   "source": "",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   }
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}